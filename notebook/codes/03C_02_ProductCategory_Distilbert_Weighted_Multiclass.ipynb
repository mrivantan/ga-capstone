{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(string, ref):\n",
    "     return ref.index(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv\n",
    "df = pd.read_csv('../datasets/cleaned/combined_text.csv')\n",
    "\n",
    "# drop duplicates\n",
    "df.drop_duplicates(subset=['combined_text'], keep='first', ignore_index=True, inplace=True)\n",
    "\n",
    "# encode category\n",
    "category_list = list(df.category.unique())\n",
    "df['label'] = df['category'].map(lambda x: encode_cat(x, category_list))\n",
    "\n",
    "df.drop(columns=['asin'], inplace=True)\n",
    "df.rename(columns={'combined_text': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.label[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1271009, 3)\n",
      "TRAIN Dataset: (1143908, 3)\n",
      "TEST Dataset: (127101, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.9\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.to_csv('../datasets/cleaned/test_set_for_evaluating_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, \n",
    "# by adding a drop out and a dense layer on top of distil bert \n",
    "# to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        \n",
    "        # set the shape = (768, number_of_classes)\n",
    "        self.classifier = torch.nn.Linear(768, 21)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "\n",
    "class_values = df['label'].value_counts(sort=False)\n",
    "weights = class_values.sum() / class_values\n",
    "normalized_weights = list(weights/weights.sum())\n",
    "class_weights = torch.FloatTensor(normalized_weights).cuda()\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch, loader = training_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mriva\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 3.047241687774658\n",
      "Training Accuracy per 5000 steps: 0.0\n",
      "Training Loss per 5000 steps: 2.914866801620221\n",
      "Training Accuracy per 5000 steps: 17.98640271945611\n",
      "Training Loss per 5000 steps: 2.813370509822778\n",
      "Training Accuracy per 5000 steps: 22.835216478352166\n",
      "Training Loss per 5000 steps: 2.7174937577893212\n",
      "Training Accuracy per 5000 steps: 26.441570561962536\n",
      "Training Loss per 5000 steps: 2.61451882978188\n",
      "Training Accuracy per 5000 steps: 29.357282135893204\n",
      "Training Loss per 5000 steps: 2.531974638333526\n",
      "Training Accuracy per 5000 steps: 31.606735730570776\n",
      "Training Loss per 5000 steps: 2.4622544852379047\n",
      "Training Accuracy per 5000 steps: 33.393886870437655\n",
      "Training Loss per 5000 steps: 2.4014447268517625\n",
      "Training Accuracy per 5000 steps: 34.904717008085484\n",
      "Training Loss per 5000 steps: 2.3518925433491042\n",
      "Training Accuracy per 5000 steps: 36.12659683507912\n",
      "Training Loss per 5000 steps: 2.3065135209470355\n",
      "Training Accuracy per 5000 steps: 37.3597253394369\n",
      "Training Loss per 5000 steps: 2.2654467913426104\n",
      "Training Accuracy per 5000 steps: 38.41023179536409\n",
      "Training Loss per 5000 steps: 2.22696172384824\n",
      "Training Accuracy per 5000 steps: 39.41928328575844\n",
      "Training Loss per 5000 steps: 2.1932146767305922\n",
      "Training Accuracy per 5000 steps: 40.28766187230213\n",
      "Training Loss per 5000 steps: 2.162493879179052\n",
      "Training Accuracy per 5000 steps: 41.12705958369871\n",
      "Training Loss per 5000 steps: 2.1334937775792255\n",
      "Training Accuracy per 5000 steps: 41.880473136098054\n",
      "Training Loss per 5000 steps: 2.107185812735929\n",
      "Training Accuracy per 5000 steps: 42.54343275422995\n",
      "Training Loss per 5000 steps: 2.0815396244278985\n",
      "Training Accuracy per 5000 steps: 43.16571042861964\n",
      "Training Loss per 5000 steps: 2.0591588273957893\n",
      "Training Accuracy per 5000 steps: 43.73683839013658\n",
      "Training Loss per 5000 steps: 2.038523061432459\n",
      "Training Accuracy per 5000 steps: 44.27311918756458\n",
      "Training Loss per 5000 steps: 2.019400945640485\n",
      "Training Accuracy per 5000 steps: 44.74952895232682\n",
      "Training Loss per 5000 steps: 2.000984987308662\n",
      "Training Accuracy per 5000 steps: 45.205047949520505\n",
      "Training Loss per 5000 steps: 1.9826747294183256\n",
      "Training Accuracy per 5000 steps: 45.68289825811183\n",
      "Training Loss per 5000 steps: 1.9658550850078067\n",
      "Training Accuracy per 5000 steps: 46.12435341496896\n",
      "Training Loss per 5000 steps: 1.9503724764536876\n",
      "Training Accuracy per 5000 steps: 46.527204111268595\n",
      "Training Loss per 5000 steps: 1.9359012335910954\n",
      "Training Accuracy per 5000 steps: 46.8837759685336\n",
      "Training Loss per 5000 steps: 1.9228388383437875\n",
      "Training Accuracy per 5000 steps: 47.234622123023016\n",
      "Training Loss per 5000 steps: 1.9094744631483798\n",
      "Training Accuracy per 5000 steps: 47.56001876908639\n",
      "Training Loss per 5000 steps: 1.89682797427817\n",
      "Training Accuracy per 5000 steps: 47.86112695461515\n",
      "Training Loss per 5000 steps: 1.884930928316306\n",
      "Training Accuracy per 5000 steps: 48.1646559667431\n",
      "Training Loss per 5000 steps: 1.873703513066652\n",
      "Training Accuracy per 5000 steps: 48.448803801353094\n",
      "Training Loss per 5000 steps: 1.863535497631081\n",
      "Training Accuracy per 5000 steps: 48.7130085799428\n",
      "Training Loss per 5000 steps: 1.853390718062305\n",
      "Training Accuracy per 5000 steps: 48.96984535583641\n",
      "Training Loss per 5000 steps: 1.844154523467495\n",
      "Training Accuracy per 5000 steps: 49.2037549765314\n",
      "Training Loss per 5000 steps: 1.8341051072792727\n",
      "Training Accuracy per 5000 steps: 49.44485184938273\n",
      "Training Loss per 5000 steps: 1.8259767647672227\n",
      "Training Accuracy per 5000 steps: 49.654854971441345\n",
      "Training Loss per 5000 steps: 1.8170439651839039\n",
      "Training Accuracy per 5000 steps: 49.87800069713887\n",
      "Training Loss per 5000 steps: 1.8081495598433308\n",
      "Training Accuracy per 5000 steps: 50.099027227626515\n",
      "Training Loss per 5000 steps: 1.7998164492492181\n",
      "Training Accuracy per 5000 steps: 50.31702531337668\n",
      "Training Loss per 5000 steps: 1.7920737041684072\n",
      "Training Accuracy per 5000 steps: 50.51499728948795\n",
      "Training Loss per 5000 steps: 1.7844996462229397\n",
      "Training Accuracy per 5000 steps: 50.6919195286178\n",
      "Training Loss per 5000 steps: 1.7772222781119655\n",
      "Training Accuracy per 5000 steps: 50.88187059064705\n",
      "Training Loss per 5000 steps: 1.7700667054983947\n",
      "Training Accuracy per 5000 steps: 51.04243393934664\n",
      "Training Loss per 5000 steps: 1.7631045872109217\n",
      "Training Accuracy per 5000 steps: 51.21427993200032\n",
      "Training Loss per 5000 steps: 1.7567104860945342\n",
      "Training Accuracy per 5000 steps: 51.36522620825019\n",
      "Training Loss per 5000 steps: 1.7503887575112824\n",
      "Training Accuracy per 5000 steps: 51.51601583629165\n",
      "Training Loss per 5000 steps: 1.7442650511841988\n",
      "Training Accuracy per 5000 steps: 51.674881444971355\n",
      "Training Loss per 5000 steps: 1.7383125168270452\n",
      "Training Accuracy per 5000 steps: 51.83260072782292\n",
      "Training Loss per 5000 steps: 1.7321303811820703\n",
      "Training Accuracy per 5000 steps: 51.97903413176965\n",
      "Training Loss per 5000 steps: 1.7261757585883857\n",
      "Training Accuracy per 5000 steps: 52.127907800384165\n",
      "Training Loss per 5000 steps: 1.720520392936229\n",
      "Training Accuracy per 5000 steps: 52.25243978596006\n",
      "Training Loss per 5000 steps: 1.7146886819985918\n",
      "Training Accuracy per 5000 steps: 52.386090455638175\n",
      "Training Loss per 5000 steps: 1.7094724142340463\n",
      "Training Accuracy per 5000 steps: 52.51734306924286\n",
      "Training Loss per 5000 steps: 1.704298074202394\n",
      "Training Accuracy per 5000 steps: 52.65345133287949\n",
      "Training Loss per 5000 steps: 1.6996363810946007\n",
      "Training Accuracy per 5000 steps: 52.774329153474895\n",
      "Training Loss per 5000 steps: 1.6953602833763972\n",
      "Training Accuracy per 5000 steps: 52.878415265128645\n",
      "Training Loss per 5000 steps: 1.6908268698435707\n",
      "Training Accuracy per 5000 steps: 52.99817091574212\n",
      "Training Loss per 5000 steps: 1.6865814705232076\n",
      "Training Accuracy per 5000 steps: 53.108203184988625\n",
      "Training Loss per 5000 steps: 1.682246514216776\n",
      "Training Accuracy per 5000 steps: 53.21858519794667\n",
      "The Total Accuracy for Epoch 0: 53.23977103053742\n",
      "Training Loss Epoch: 1.6813222516079211\n",
      "Training Accuracy Epoch: 53.23977103053742\n",
      "Wall time: 19h 13min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "# Saving evaluation model\n",
    "\n",
    "output_model_file = '../models/pytorch_distilbert_amazon_weighted_eval.bin'\n",
    "output_vocab_file = '../models/vocab_distilbert_amazon_weighted_eval.bin'\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('All files saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    tr_loss=0\n",
    "    nb_tr_steps=0\n",
    "    nb_tr_examples=0\n",
    "    \n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "#             outputs = model(ids, mask).squeeze()\n",
    "            outputs = model(ids, mask)\n",
    "            \n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 0.10859662294387817\n",
      "Validation Accuracy per 100 steps: 100.0\n",
      "Validation Loss per 100 steps: 1.3412874694455508\n",
      "Validation Accuracy per 100 steps: 61.67766446710658\n",
      "Validation Loss per 100 steps: 1.347678253743347\n",
      "Validation Accuracy per 100 steps: 61.8988101189881\n",
      "Validation Loss per 100 steps: 1.352023130318458\n",
      "Validation Accuracy per 100 steps: 61.289247383507764\n",
      "Validation Loss per 100 steps: 1.3493625808715903\n",
      "Validation Accuracy per 100 steps: 61.314434278286086\n",
      "Validation Loss per 100 steps: 1.3417594904380987\n",
      "Validation Accuracy per 100 steps: 61.47954081836726\n",
      "Validation Loss per 100 steps: 1.3404679150292775\n",
      "Validation Accuracy per 100 steps: 61.519616012799574\n",
      "Validation Loss per 100 steps: 1.3355486017672558\n",
      "Validation Accuracy per 100 steps: 61.57395502985629\n",
      "Validation Loss per 100 steps: 1.3321532088558696\n",
      "Validation Accuracy per 100 steps: 61.69720756981076\n",
      "Validation Loss per 100 steps: 1.3376154334080244\n",
      "Validation Accuracy per 100 steps: 61.570853981022644\n",
      "Validation Loss per 100 steps: 1.337877276144423\n",
      "Validation Accuracy per 100 steps: 61.572768544629106\n",
      "Validation Loss per 100 steps: 1.3372839129227998\n",
      "Validation Accuracy per 100 steps: 61.615243359211654\n",
      "Validation Loss per 100 steps: 1.338696246477729\n",
      "Validation Accuracy per 100 steps: 61.599806669888835\n",
      "Validation Loss Epoch: 1.3372389748150655\n",
      "Validation Accuracy Epoch: 61.60769781512341\n",
      "Accuracy on test data = 61.60769781512341\n"
     ]
    }
   ],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(f\"Accuracy on test data = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning with rest of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.0659838318824768\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "Training Loss per 5000 steps: 1.3968120152724788\n",
      "Training Accuracy per 5000 steps: 60.80783843231354\n",
      "Training Loss per 5000 steps: 1.3896202186294628\n",
      "Training Accuracy per 5000 steps: 61.48885111488851\n",
      "Training Loss per 5000 steps: 1.3861922403933098\n",
      "Training Accuracy per 5000 steps: 61.49256716218919\n",
      "Training Loss per 5000 steps: 1.389867887752088\n",
      "Training Accuracy per 5000 steps: 61.116944152792364\n",
      "Training Loss per 5000 steps: 1.3882942344849152\n",
      "Training Accuracy per 5000 steps: 61.23355065797368\n",
      "Training Loss per 5000 steps: 1.391144970667556\n",
      "Training Accuracy per 5000 steps: 61.26295790140329\n",
      "Training Loss per 5000 steps: 1.392937280102996\n",
      "Training Accuracy per 5000 steps: 61.1996800091426\n",
      "Training Loss per 5000 steps: 1.390451846507463\n",
      "Training Accuracy per 5000 steps: 61.26721831954201\n",
      "Training Loss per 5000 steps: 1.3890476740060247\n",
      "Training Accuracy per 5000 steps: 61.39530232661497\n",
      "Training Loss per 5000 steps: 1.38756240965435\n",
      "Training Accuracy per 5000 steps: 61.46477070458591\n",
      "Training Loss per 5000 steps: 1.38589950910876\n",
      "Training Accuracy per 5000 steps: 61.44433737568408\n",
      "Training Loss per 5000 steps: 1.3863372082247183\n",
      "Training Accuracy per 5000 steps: 61.4106431559474\n",
      "The Total Accuracy for Epoch 0: 61.4983359690325\n",
      "Training Loss Epoch: 1.3835224707620863\n",
      "Training Accuracy Epoch: 61.4983359690325\n",
      "Wall time: 2h 34min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, loader=testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model Artifacts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "# Saving fully trained model\n",
    "\n",
    "output_model_file = '../models/pytorch_distilbert_amazon_weighted_full.bin'\n",
    "output_vocab_file = '../models/vocab_distilbert_amazon_weighted_full.bin'\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('All files saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
