{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(string, ref):\n",
    "     return ref.index(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv\n",
    "df = pd.read_csv('../datasets/cleaned/combined_text.csv')\n",
    "\n",
    "# drop duplicates\n",
    "df.drop_duplicates(subset=['combined_text'], keep='first', ignore_index=True, inplace=True)\n",
    "\n",
    "# encode category\n",
    "category_list = list(df.category.unique())\n",
    "df['label'] = df['category'].map(lambda x: encode_cat(x, category_list))\n",
    "\n",
    "df.drop(columns=['asin'], inplace=True)\n",
    "df.rename(columns={'combined_text': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.label[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1271009, 3)\n",
      "TRAIN Dataset: (1143908, 3)\n",
      "TEST Dataset: (127101, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.9\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, \n",
    "# by adding a drop out and a dense layer on top of distil bert \n",
    "# to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 21)\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch, loader = training_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mriva\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 3.032930850982666\n",
      "Training Accuracy per 5000 steps: 0.0\n",
      "Training Loss per 5000 steps: 2.448926548163096\n",
      "Training Accuracy per 5000 steps: 28.144371125774846\n",
      "Training Loss per 5000 steps: 2.3054151399223675\n",
      "Training Accuracy per 5000 steps: 32.62423757624238\n",
      "Training Loss per 5000 steps: 2.1996813833417517\n",
      "Training Accuracy per 5000 steps: 35.880941270581964\n",
      "Training Loss per 5000 steps: 2.122731206913387\n",
      "Training Accuracy per 5000 steps: 38.169341532923355\n",
      "Training Loss per 5000 steps: 2.056580785453179\n",
      "Training Accuracy per 5000 steps: 40.22439102435902\n",
      "Training Loss per 5000 steps: 1.9986112759335313\n",
      "Training Accuracy per 5000 steps: 41.927769074364186\n",
      "Training Loss per 5000 steps: 1.9539980673242652\n",
      "Training Accuracy per 5000 steps: 43.23019342304506\n",
      "Training Loss per 5000 steps: 1.9125094847636626\n",
      "Training Accuracy per 5000 steps: 44.43326416839579\n",
      "Training Loss per 5000 steps: 1.875731753826748\n",
      "Training Accuracy per 5000 steps: 45.49621119530677\n",
      "Training Loss per 5000 steps: 1.8449356811060869\n",
      "Training Accuracy per 5000 steps: 46.40057198856023\n",
      "Training Loss per 5000 steps: 1.8169531506003636\n",
      "Training Accuracy per 5000 steps: 47.18186942055599\n",
      "Training Loss per 5000 steps: 1.7917855107129474\n",
      "Training Accuracy per 5000 steps: 47.899618339694335\n",
      "Training Loss per 5000 steps: 1.768687740815475\n",
      "Training Accuracy per 5000 steps: 48.56348363871325\n",
      "Training Loss per 5000 steps: 1.7460017338562968\n",
      "Training Accuracy per 5000 steps: 49.19786860187712\n",
      "Training Loss per 5000 steps: 1.7268359292890074\n",
      "Training Accuracy per 5000 steps: 49.73200357328569\n",
      "Training Loss per 5000 steps: 1.7099721210890881\n",
      "Training Accuracy per 5000 steps: 50.196560042999465\n",
      "Training Loss per 5000 steps: 1.6925868653832101\n",
      "Training Accuracy per 5000 steps: 50.68322725614993\n",
      "Training Loss per 5000 steps: 1.6778813995498916\n",
      "Training Accuracy per 5000 steps: 51.0883212408751\n",
      "Training Loss per 5000 steps: 1.6633752399593968\n",
      "Training Accuracy per 5000 steps: 51.48893169545584\n",
      "Training Loss per 5000 steps: 1.6499764283644431\n",
      "Training Accuracy per 5000 steps: 51.843731562684376\n",
      "Training Loss per 5000 steps: 1.6378881127633713\n",
      "Training Accuracy per 5000 steps: 52.19164579384958\n",
      "Training Loss per 5000 steps: 1.6266166031594438\n",
      "Training Accuracy per 5000 steps: 52.51452259524913\n",
      "Training Loss per 5000 steps: 1.6149158444441174\n",
      "Training Accuracy per 5000 steps: 52.83649707393849\n",
      "Training Loss per 5000 steps: 1.6043368829755187\n",
      "Training Accuracy per 5000 steps: 53.13226556445363\n",
      "Training Loss per 5000 steps: 1.5937899594834286\n",
      "Training Accuracy per 5000 steps: 53.42897256821946\n",
      "Training Loss per 5000 steps: 1.5841576336001046\n",
      "Training Accuracy per 5000 steps: 53.69804847655018\n",
      "Training Loss per 5000 steps: 1.5749542026601298\n",
      "Training Accuracy per 5000 steps: 53.94571151324805\n",
      "Training Loss per 5000 steps: 1.56584964714428\n",
      "Training Accuracy per 5000 steps: 54.19675573745902\n",
      "Training Loss per 5000 steps: 1.557055425811437\n",
      "Training Accuracy per 5000 steps: 54.43979696691747\n",
      "Training Loss per 5000 steps: 1.5489962826346364\n",
      "Training Accuracy per 5000 steps: 54.65363564242905\n",
      "Training Loss per 5000 steps: 1.5417536809444214\n",
      "Training Accuracy per 5000 steps: 54.84303327075309\n",
      "Training Loss per 5000 steps: 1.5341675543645155\n",
      "Training Accuracy per 5000 steps: 55.05231217304892\n",
      "Training Loss per 5000 steps: 1.5265620499143777\n",
      "Training Accuracy per 5000 steps: 55.253756037842194\n",
      "Training Loss per 5000 steps: 1.5197051585315078\n",
      "Training Accuracy per 5000 steps: 55.44202681160699\n",
      "Training Loss per 5000 steps: 1.5122766440810358\n",
      "Training Accuracy per 5000 steps: 55.64611059365375\n",
      "Training Loss per 5000 steps: 1.5056690194854963\n",
      "Training Accuracy per 5000 steps: 55.81885656190799\n",
      "Training Loss per 5000 steps: 1.4994675721231248\n",
      "Training Accuracy per 5000 steps: 55.98550818644224\n",
      "Training Loss per 5000 steps: 1.4930999079390843\n",
      "Training Accuracy per 5000 steps: 56.15641496623702\n",
      "Training Loss per 5000 steps: 1.4871005021496286\n",
      "Training Accuracy per 5000 steps: 56.317788113907106\n",
      "Training Loss per 5000 steps: 1.4813464826927878\n",
      "Training Accuracy per 5000 steps: 56.47934260328698\n",
      "Training Loss per 5000 steps: 1.475387238314419\n",
      "Training Accuracy per 5000 steps: 56.644357832400814\n",
      "Training Loss per 5000 steps: 1.4699785400401322\n",
      "Training Accuracy per 5000 steps: 56.79472954890691\n",
      "Training Loss per 5000 steps: 1.4645157251147944\n",
      "Training Accuracy per 5000 steps: 56.95124673838726\n",
      "Training Loss per 5000 steps: 1.4594549979443088\n",
      "Training Accuracy per 5000 steps: 57.09371775582838\n",
      "Training Loss per 5000 steps: 1.4545181420521318\n",
      "Training Accuracy per 5000 steps: 57.22707899076004\n",
      "Training Loss per 5000 steps: 1.4499477635368612\n",
      "Training Accuracy per 5000 steps: 57.350946300233474\n",
      "Training Loss per 5000 steps: 1.4454076134120797\n",
      "Training Accuracy per 5000 steps: 57.48486176654568\n",
      "Training Loss per 5000 steps: 1.441005154473688\n",
      "Training Accuracy per 5000 steps: 57.60736413598276\n",
      "Training Loss per 5000 steps: 1.436685017573935\n",
      "Training Accuracy per 5000 steps: 57.717927681927826\n",
      "Training Loss per 5000 steps: 1.4323870230593354\n",
      "Training Accuracy per 5000 steps: 57.832268670925316\n",
      "Training Loss per 5000 steps: 1.4281953449094187\n",
      "Training Accuracy per 5000 steps: 57.95055705663899\n",
      "Training Loss per 5000 steps: 1.423987278143342\n",
      "Training Accuracy per 5000 steps: 58.06285360440921\n",
      "Training Loss per 5000 steps: 1.4201548218074265\n",
      "Training Accuracy per 5000 steps: 58.164780510262226\n",
      "Training Loss per 5000 steps: 1.415995084790828\n",
      "Training Accuracy per 5000 steps: 58.2797841489476\n",
      "Training Loss per 5000 steps: 1.4124946388664794\n",
      "Training Accuracy per 5000 steps: 58.373605914160315\n",
      "Training Loss per 5000 steps: 1.4085644051378223\n",
      "Training Accuracy per 5000 steps: 58.47970185820765\n",
      "Training Loss per 5000 steps: 1.4051077941542809\n",
      "Training Accuracy per 5000 steps: 58.57102255781559\n",
      "The Total Accuracy for Epoch 0: 58.58574290939481\n",
      "Training Loss Epoch: 1.4045782524416772\n",
      "Training Accuracy Epoch: 58.58574290939481\n",
      "Wall time: 19h 13min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "# Saving fully trained model\n",
    "\n",
    "output_model_file = '../models/pytorch_distilbert_amazon_imbalanced_eval.bin'\n",
    "output_vocab_file = '../models/vocab_distilbert_amazon_imbalanced_eval.bin'\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('All files saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    tr_loss=0\n",
    "    nb_tr_steps=0\n",
    "    nb_tr_examples=0\n",
    "    \n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "#             outputs = model(ids, mask).squeeze()\n",
    "            outputs = model(ids, mask)\n",
    "            \n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the validation section to print the accuracy and see how it performs\n",
      "Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch\n",
      "Validation Loss per 100 steps: 1.089339256286621\n",
      "Validation Accuracy per 100 steps: 50.0\n",
      "Validation Loss per 100 steps: 1.170224219082764\n",
      "Validation Accuracy per 100 steps: 65.06698660267946\n",
      "Validation Loss per 100 steps: 1.1618767426983483\n",
      "Validation Accuracy per 100 steps: 65.1934806519348\n",
      "Validation Loss per 100 steps: 1.1616851459648434\n",
      "Validation Accuracy per 100 steps: 65.21898540097327\n",
      "Validation Loss per 100 steps: 1.1670875136965082\n",
      "Validation Accuracy per 100 steps: 64.95175241237938\n",
      "Validation Loss per 100 steps: 1.1664611940916378\n",
      "Validation Accuracy per 100 steps: 65.00139994400224\n",
      "Validation Loss per 100 steps: 1.1682161804553919\n",
      "Validation Accuracy per 100 steps: 64.96450118329389\n",
      "Validation Loss per 100 steps: 1.1694670404466903\n",
      "Validation Accuracy per 100 steps: 65.01814233879033\n",
      "Validation Loss per 100 steps: 1.1718934575639977\n",
      "Validation Accuracy per 100 steps: 64.92462688432789\n",
      "Validation Loss per 100 steps: 1.170133352457428\n",
      "Validation Accuracy per 100 steps: 64.92744605675429\n",
      "Validation Loss per 100 steps: 1.169698418262868\n",
      "Validation Accuracy per 100 steps: 64.98570028599428\n",
      "Validation Loss per 100 steps: 1.1687296816403405\n",
      "Validation Accuracy per 100 steps: 64.94427374047744\n",
      "Validation Loss per 100 steps: 1.1691524776932212\n",
      "Validation Accuracy per 100 steps: 64.90391826802886\n",
      "Validation Loss Epoch: 1.1689901190696705\n",
      "Validation Accuracy Epoch: 64.890913525464\n",
      "Accuracy on test data = 64.890913525464\n"
     ]
    }
   ],
   "source": [
    "print('This is the validation section to print the accuracy and see how it performs')\n",
    "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
    "\n",
    "acc = valid(model, testing_loader)\n",
    "print(f\"Accuracy on test data = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning with rest of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 2.310896873474121\n",
      "Training Accuracy per 5000 steps: 50.0\n",
      "Training Loss per 5000 steps: 1.2152239886251996\n",
      "Training Accuracy per 5000 steps: 63.6372725454909\n",
      "Training Loss per 5000 steps: 1.2118654553385957\n",
      "Training Accuracy per 5000 steps: 63.81861813818618\n",
      "Training Loss per 5000 steps: 1.2042404591154863\n",
      "Training Accuracy per 5000 steps: 64.12905806279582\n",
      "Training Loss per 5000 steps: 1.207519852453522\n",
      "Training Accuracy per 5000 steps: 63.974301284935756\n",
      "Training Loss per 5000 steps: 1.210596567041873\n",
      "Training Accuracy per 5000 steps: 63.821447142114316\n",
      "Training Loss per 5000 steps: 1.2124395968648225\n",
      "Training Accuracy per 5000 steps: 63.74787507083097\n",
      "Training Loss per 5000 steps: 1.2099000658072925\n",
      "Training Accuracy per 5000 steps: 63.86674666438102\n",
      "Training Loss per 5000 steps: 1.2115825196247887\n",
      "Training Accuracy per 5000 steps: 63.8321541961451\n",
      "Training Loss per 5000 steps: 1.2127715741149132\n",
      "Training Accuracy per 5000 steps: 63.724139463567475\n",
      "Training Loss per 5000 steps: 1.2122565453115457\n",
      "Training Accuracy per 5000 steps: 63.73872522549549\n",
      "Training Loss per 5000 steps: 1.2138155845572873\n",
      "Training Accuracy per 5000 steps: 63.670660533444845\n",
      "Training Loss per 5000 steps: 1.2134472973296284\n",
      "Training Accuracy per 5000 steps: 63.69310511491475\n",
      "The Total Accuracy for Epoch 0: 63.68321256323711\n",
      "Training Loss Epoch: 1.2132725318697468\n",
      "Training Accuracy Epoch: 63.68321256323711\n",
      "Wall time: 2h 34min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, loader=testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model Artifacts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "# Saving fully trained model\n",
    "\n",
    "output_model_file = '../models/pytorch_distilbert_amazon_imbalanced_full.bin'\n",
    "output_vocab_file = '../models/vocab_distilbert_amazon_imbalanced_full.bin'\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('All files saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
